\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Data Mining: Learning from Large Data Sets - Fall Semester 2015}
\author{caifa.zhou@geod.baug.ethz.ch\\ pungast@student.ethz.ch\\ llara@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Extracting Representative Elements} 

To extract 100 clusters of representative elements from a large dataset, we used k-means ++ and k-means in the mapper and reducer, where the mapper outputs 500 points (cluster centers) for each batch and the reducer finds the final 100 based on the output of the mappers.

The \textbf{mapper} is constructed as follows. Data is processed in batches of 10000 data points.

The cluster centers of each stream are initialised using k-means++ to find a solution in reasonable time, and then sequential k-means is run. Weights are assigned to all points, whereby points farther away from the existing centers get a higher weight and thus have a higher probability to be selected. Iteratively the subsequent centers are chosen from the remaining data points with probability proportional to its squared distance ($\left \| x_{i} - \mu_{j} \right \|^{2}_{2}$) from the closest existing cluster center $\mu_{j}$.

Third, the sequential k-means algorithm is implemented to compute representative elements ($\mu_{j}$) one at a time. The centers (means) are given by the vector $\mu$ with $\mu_{1},...,\mu_{k}$, the algorithm calculates $\partial L/\partial \mu$ with $ \partial L(x,\mu) = min \left \| x_{i} - \mu_{j} \right \|^{2}_{2}$. If $\mu_{i}$ is closest to x,  $\mu_{i}$ is replaced by $\mu_{i}  + \alpha \times (x - \mu_{i})$ with step size  $\alpha \in (0,1)$. 
Last, the emit function prints the centers into stdout.

The \textbf{reducer} receives the cluster centers from all mappers. As in the mapper, the reducer finds runs k-means++ in this reduced space to find initial cluster centers. Sequential k-means algorithm is then run on these initial centers to find the final representative elements. \\ 

\end{document} 
